âœ… Tam Ä°Ã§erik â€” docs/llama_cpp_notes.md
markdown
Kopyala
DÃ¼zenle
# ğŸ§  REBEL LLM - llama.cpp NotlarÄ±

## ğŸ“¦ Derleme SonrasÄ± Binary Konumu
Derlemeden sonra binary dosyalarÄ± ÅŸu klasÃ¶re yerleÅŸir:
rebel_llm/external/llama.cpp/build/bin/Release/

perl
Kopyala
DÃ¼zenle

## ğŸš€ Ã–nemli Binary'ler
- `llama-simple.exe`: Temel sohbet baÅŸlatÄ±cÄ± (chat_interface.py tarafÄ±ndan kullanÄ±lÄ±r).
- `main.exe`: CLI Ã¼zerinden giriÅŸ yapÄ±labilen temel model Ã§alÄ±ÅŸma aracÄ±.
- `quantize.exe`: GGUF modeli quantize etmek iÃ§in kullanÄ±lÄ±r.
- `benchmark.exe`: Model hÄ±z testlerini yapar.

## ğŸ—‚ï¸ Model KullanÄ±mÄ±
`.env` iÃ§inde model yolunu belirt:
MODEL_PATH=models/gemma-1.1-2b-it.Q4_K_M.gguf

bash
Kopyala
DÃ¼zenle

## ğŸ’¡ Komut Ã–rnekleri
**Test AmaÃ§lÄ± Ã‡alÄ±ÅŸtÄ±rma:**
```bash
./llama-simple.exe -m models/gemma-1.1-2b-it.Q4_K_M.gguf
Quantize Etmek Ä°Ã§in:

bash
Kopyala
DÃ¼zenle
./quantize.exe models/original.gguf models/quantized.Q4_K_M.gguf Q4_K_M
âš™ï¸ CMake NotlarÄ±
EÄŸer curl problemi varsa:

pgsql
Kopyala
DÃ¼zenle
cmake .. -DLLAMA_CURL=OFF -G "Visual Studio 17 2022" -A x64 -DCMAKE_BUILD_TYPE=Release
ğŸ“Œ BaÄŸlantÄ±lÄ± Dosyalar
chat_interface.py â†’ llama-simple.exe Ã¼zerinden modelle sohbet baÅŸlatÄ±r.

benchmark_runner.py â†’ benchmark.exe ile test yapar.

.env â†’ MODEL_PATH buradan alÄ±nÄ±r.

Not: Bu dokÃ¼man, geliÅŸtiricinin sistemi daha hÄ±zlÄ± kavramasÄ± iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r. Otomatik kurulumdan sonra binary yolu deÄŸiÅŸirse .env gÃ¼ncellenmelidir.